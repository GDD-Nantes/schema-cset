{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46228-6078-4bf3-b3d6-8ac13048e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook calculate the characterstic sets of two datasets and keeps their distinct domain count, then it compares the evolution of each of their data points\n",
    "# -> count, coverage, average, dinctinct domain count\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot\n",
    "\n",
    "# Create a local SparkSession (seulement le master et pas les slaves)\n",
    "# spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate() \n",
    "\n",
    "#créer une session dans le master\n",
    "'''spark = SparkSession.builder \\\n",
    "    .master(\"spark://172.20.53.96:7077\") \\\n",
    "    .appName(\"WDC-NouveauExemplePetitGraph\") \\\n",
    "    .config(\"spark.executor.memory\",\"28g\") \\\n",
    "    .config(\"spark.driver.memory\",\"28g\") \\\n",
    "    .getOrCreate()'''\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"WDC-NouveauExemplePetitGraph\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",1000)\n",
    "\n",
    "#fichiers de config qui permettent de se connecter au serveur de stockage s3 qui contient les fichiers de DataCommons\n",
    "endpoint_url = 'https://s3.os-bird.glicid.fr/'\n",
    "aws_access_key_id = 'bbd95ea3c1174caa88345404b84e458f'\n",
    "aws_secret_access_key = 'eaf2a72ecf9845f583af7f3513c44f25'\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', aws_access_key_id)\n",
    "hadoopConf.set('fs.s3a.secret.key', aws_secret_access_key)\n",
    "hadoopConf.set('fs.s3a.endpoint', endpoint_url)\n",
    "hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "hadoopConf.set('spark.worker.cleanup.enabled', 'true')\n",
    "hadoopConf.set('fs.s3a.committer.name', 'magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f61a9-7462-4bbc-b825-41122b9242ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import tldextract\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "quad_motif = re.compile(r'([^\\s]+)\\s([^\\s]+)\\s(.+)\\s([^\\s]+)\\s+\\.')\n",
    "\n",
    "\n",
    "def parseQ(l, parts):\n",
    "    result=quad_motif.match(l)\n",
    "    #print(result.groups())\n",
    "    sub=result.group(1).strip()\n",
    "    pred=result.group(2).strip()\n",
    "    if pred==\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>\":\n",
    "        pred=\"isa:\"+result.group(3).strip()\n",
    "  \n",
    "    if sub.startswith(\"_:\"):\n",
    "        sub += result.group(4).strip().strip(\"<>\")\n",
    "        \n",
    "    #print(hashstring)\n",
    "    return Row(subject=sub,predicate=pred,hashdom=hash(sub)%parts)\n",
    "\n",
    "\n",
    "# fonction qui extrait les top-level domains ou l'ip d'une string\n",
    "@udf('string')\n",
    "def extract_tld(url):\n",
    "    tld = tldextract.extract(url)\n",
    "    if tld.registered_domain:\n",
    "        return tld.registered_domain\n",
    "    else:\n",
    "        # in case the URL is an IP\n",
    "        return tld.domain\n",
    "\n",
    "spark.udf.register(\"extract_tld\", extract_tld)\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import random \n",
    "\n",
    "# selectionne aléatoirement num urls dans un array\n",
    "@udf(ArrayType(StringType()))\n",
    "def sample_tab(input_array, num):\n",
    "    # Vérifiez si l'entrée est une liste non vide\n",
    "    if input_array and isinstance(input_array, list):\n",
    "        # Utilisez random.sample pour sélectionner aléatoirement 10 éléments sans remplacement\n",
    "        selected_elements = random.sample(input_array, min(10, len(input_array)))\n",
    "        return selected_elements\n",
    "    else:\n",
    "        return None  # Retournez None si l'entrée n'est pas valide\n",
    "\n",
    "spark.udf.register(\"sample_tab\", sample_tab)\n",
    "\n",
    "parts = 1\n",
    "# File file:/webdatacommons_data/home/nathan/petitGrapheRDFavant.txt does not exist\n",
    "#spark.sparkContext.addFile(\"petitGrapheRDFavant.txt\")\n",
    "linesavant = spark.sparkContext.textFile(f\"petitGrapheRDFavant.txt\")\n",
    "\n",
    "spavant=linesavant.map(lambda l: parseQ(l, parts)).toDF()\n",
    "spavant.createOrReplaceTempView(\"Superavant\")\n",
    "\n",
    "# We compute the CSET with a list of the disctincts domains, the count of them and a sample of 10 subjects on the before and the after graph\n",
    "resultavant = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            pset,\n",
    "            COUNT(subject) as count,\n",
    "            COLLECT_SET(extract_tld(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1'))) as distinct_domains,\n",
    "            SIZE(COLLECT_SET(extract_tld(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1')))) as num_distinct_domains,\n",
    "            sample_tab(COLLECT_LIST(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1')), 10) as urls\n",
    "        FROM (\n",
    "            SELECT\n",
    "                subject,\n",
    "                CONCAT_WS(' ', SORT_ARRAY(COLLECT_SET(predicate))) as pset\n",
    "            FROM\n",
    "                Superavant\n",
    "            GROUP BY\n",
    "                subject\n",
    "        )\n",
    "        GROUP BY\n",
    "            pset\n",
    "    \"\"\").cache()\n",
    "\n",
    "resultavant.createOrReplaceTempView(\"CSET_avant\")\n",
    "\n",
    "#spark.sparkContext.addFile(\"petitGrapheRDFapres.txt\")\n",
    "linesapres = spark.sparkContext.textFile(f\"petitGrapheRDFapres.txt\")\n",
    "\n",
    "spapres=linesapres.map(lambda l: parseQ(l, parts)).toDF()\n",
    "spapres.createOrReplaceTempView(\"Superapres\")\n",
    "\n",
    "resultapres = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            pset,\n",
    "            COUNT(subject) as count,\n",
    "            COLLECT_SET(extract_tld(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1'))) as distinct_domains,\n",
    "            SIZE(COLLECT_SET(extract_tld(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1')))) as num_distinct_domains,\n",
    "            sample_tab(COLLECT_LIST(regexp_replace(subject, '^[^<>]*<(.+)>[^<>]*$', '$1')),10) as urls\n",
    "        FROM (\n",
    "            SELECT\n",
    "                subject,\n",
    "                CONCAT_WS(' ', SORT_ARRAY(COLLECT_SET(predicate))) as pset\n",
    "            FROM\n",
    "                Superapres\n",
    "            GROUP BY\n",
    "                subject\n",
    "        )\n",
    "        GROUP BY\n",
    "            pset\n",
    "    \"\"\").cache()\n",
    "\n",
    "resultapres.show(truncate=0)\n",
    "\n",
    "resultapres.createOrReplaceTempView(\"CSET_apres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b343ea4-e78c-40b3-8299-ef742cedf9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from CSET_avant\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27721b7-7466-475f-b656-bbda7956302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from CSET_apres\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da17872-c280-4527-8ab4-b3cc501906b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche l'évolution en pourcentage du nombre d'occurence de chaque caracteristic set entre le graphe avant et le graphe après\n",
    "csetCountEvolution = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COALESCE(CSET_avant.pset, CSET_apres.pset) AS pset,\n",
    "    CASE\n",
    "        WHEN CSET_avant.count IS NULL THEN 0  -- Pset didn't exist in the first dataframe\n",
    "        ELSE CSET_avant.count\n",
    "    END AS cset_count_avant,\n",
    "    CASE\n",
    "        WHEN CSET_apres.count IS NULL THEN 0  -- Pset existed before and disappeared\n",
    "        ELSE CSET_apres.count\n",
    "    END AS cset_count_apres,\n",
    "    CASE\n",
    "        WHEN CSET_avant.pset IS NULL THEN \"new\"  -- Pset didn't exist in the first dataframe\n",
    "        WHEN CSET_apres.pset IS NULL THEN -100.0    -- Pset existed before and disappeared\n",
    "        ELSE (CSET_apres.count - CSET_avant.count) / CSET_avant.count * 100.0\n",
    "    END AS percentage_count_evolution\n",
    "FROM\n",
    "    CSET_avant\n",
    "FULL OUTER JOIN\n",
    "    CSET_apres\n",
    "ON\n",
    "    CSET_avant.pset = CSET_apres.pset\n",
    "\"\"\")\n",
    "csetCountEvolution.show(truncate=0)\n",
    "\n",
    "csetCountEvolution.createOrReplaceTempView(\"CSET_number_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba0b9a-46af-425d-a89e-2180e07f8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affiche le num_distinct_domains du graphe avant et celui d'après, avec leur evolution en pourcentage,\n",
    "# et cela pour chacun des cset\n",
    "\n",
    "\n",
    "distinctDomainCountEvolution = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COALESCE(avant.pset, apres.pset) as pset,\n",
    "        MAX(COALESCE(avant.num_distinct_domains, 0)) as avant_domain_count,\n",
    "        MAX(COALESCE(apres.num_distinct_domains, 0)) as apres_domain_count,\n",
    "        CASE\n",
    "            WHEN COALESCE(MAX(avant.num_distinct_domains), 0) = 0 THEN \"new\"\n",
    "            ELSE (MAX(COALESCE(apres.num_distinct_domains, 0)) - MAX(COALESCE(avant.num_distinct_domains, 0))) / MAX(COALESCE(avant.num_distinct_domains, 0)) * 100\n",
    "        END as percentage_domain_evolution\n",
    "    FROM\n",
    "        CSET_avant avant\n",
    "    FULL OUTER JOIN\n",
    "        CSET_apres apres\n",
    "    ON\n",
    "        avant.pset = apres.pset\n",
    "    GROUP BY\n",
    "        COALESCE(avant.pset, apres.pset)\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "distinctDomainCountEvolution.show(truncate=0)\n",
    "\n",
    "distinctDomainCountEvolution.createOrReplaceTempView(\"CSET_distinct_domain_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699cd57-84a0-4ddc-a7b3-9ab6aa0acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage\n",
    "def calculate_coverage(data, cname):\n",
    "    sets=spark.sql(f\"select pset,count from {data} where pset like '%{cname}%' \")\n",
    "    sets = sets.withColumn(\"pset\", split(sets[\"pset\"], \"\\\\|\"))\n",
    "    sets.createOrReplaceTempView(\"sets\")\n",
    "    top_sets= spark.sql(\"\"\"\n",
    "    SELECT  *\n",
    "    FROM sets\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 20\n",
    "    \"\"\")\n",
    "    #top_sets.show(truncate=0)\n",
    "    \n",
    "    exploded_data = top_sets.select(\"pset\", \"count\") \\\n",
    "                    .selectExpr(\"pset\", \"count\", \"explode(pset) as predicate\")\n",
    "    distinct_predicate_count = exploded_data.select(\"predicate\") \\\n",
    "                                       .distinct() \\\n",
    "                                       .count()\n",
    "    #print(f\"distinct predicate count {distinct_predicate_count}\")\n",
    "    \n",
    "    querry = spark.sql(\"SELECT SUM(count) as count_sum FROM sets\")\n",
    "    #querry.show()\n",
    "    totalcount = querry.first()[\"count_sum\"]\n",
    "    #print(f\"total count {totalcount}\")\n",
    "    \n",
    "    \n",
    "    totalpredcount = totalcount*distinct_predicate_count\n",
    "    #print(totalpredcount)\n",
    "    \n",
    "    querry = spark.sql(\"SELECT SUM(count * size(pset)) as count_sum  FROM sets\")\n",
    "    countused = querry.first()[\"count_sum\"]\n",
    "\n",
    "    structuredness = countused/totalpredcount\n",
    "    #print(f\"coverage {pred}: {structuredness}\")\n",
    "    return structuredness\n",
    "\n",
    "def calculate_average(data, pred):\n",
    "    # Danger injection SQL\n",
    "    pred = pred.replace(\"'\", \"\\\\'\")\n",
    "    sets = spark.sql(f\"SELECT pset, count FROM {data} WHERE pset LIKE '%{pred}%'\")\n",
    "    sets = sets.withColumn(\"pset\", split(sets[\"pset\"], \" \"))\n",
    "    \n",
    "    if sets.count() <= 1:\n",
    "        count_used = sets.selectExpr(\"size(pset) * count as count_used\").collect()[0][0]\n",
    "        print(f\"average {pred}: {count_used}\")\n",
    "        return count_used\n",
    "    \n",
    "    count_sum = sets.agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "    count_used = sets.selectExpr(\"sum(size(pset) * count) as count_used\").collect()[0][0]\n",
    "\n",
    "    average = count_used / count_sum\n",
    "    #print(f\"average {pred}: {average}\")\n",
    "    return average\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "def findISA(data, topn = None):\n",
    "    if(topn is not None):\n",
    "        param = f\"limit {topn}\"\n",
    "    else:\n",
    "        param = \"\"\n",
    "    \n",
    "    csall = spark.sql(f\"select * from {data}\")\n",
    "    sets = csall.withColumn(\"pset\", split(csall[\"pset\"], \" \"))\n",
    "    \n",
    "    distinct_predicate = sets.selectExpr(\"explode(pset) as predicate\", \"count\") \\\n",
    "                    .groupBy(\"predicate\").agg(f.sum(f.col(\"count\")).alias(\"count\"))  \\\n",
    "                    .createOrReplaceTempView(\"newsets\")\n",
    "    #distinct_predicate=spark.sql(f\"select * from newsets where predicate like '%isa:%' and count > 1 order by count desc {param}\")   \n",
    "    distinct_predicate=spark.sql(f\"select * from newsets where predicate like '%isa:%' and count > 0 order by count desc {param}\")   \n",
    "    return distinct_predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45913967-613f-414a-969b-3c7c41e21869",
   "metadata": {},
   "outputs": [],
   "source": [
    "topavant = findISA(\"CSET_avant\")\n",
    "topapres = findISA(\"CSET_apres\")\n",
    "\n",
    "topavant.createOrReplaceTempView(\"toptypeavant\")\n",
    "topapres.createOrReplaceTempView(\"toptypeapres\")\n",
    "\n",
    "topavant.show(truncate=0)\n",
    "topapres.show(truncate=0)\n",
    "\n",
    "topavant_list = topavant.select('predicate').rdd.flatMap(lambda x: x).collect()\n",
    "topapres_list = topapres.select('predicate').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(len(topavant_list))\n",
    "print(len(topapres_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e1de0-2473-4bf9-a112-315dde3eeaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiche le pourcentage d'évolution du nombre d'instance pour chacun des types\n",
    "toptypeevolution = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COALESCE(toptypeavant.predicate, toptypeapres.predicate) AS type,\n",
    "    toptypeavant.count AS count_before,\n",
    "    toptypeapres.count AS count_after,\n",
    "    CASE\n",
    "        WHEN toptypeavant.predicate IS NULL THEN \"This type is only in the after graph\"  -- Type is only in toptypeapres\n",
    "        WHEN toptypeapres.predicate IS NULL THEN \"This type is only in the before graph\"  -- Type is only in toptypeavant\n",
    "        ELSE (toptypeapres.count - toptypeavant.count) / toptypeavant.count * 100.0\n",
    "    END AS percentage_count_evolution\n",
    "FROM\n",
    "    toptypeavant\n",
    "FULL OUTER JOIN\n",
    "    toptypeapres\n",
    "ON\n",
    "    toptypeavant.predicate = toptypeapres.predicate\n",
    "\"\"\")\n",
    "toptypeevolution.show(truncate=0)\n",
    "toptypeevolution.createOrReplaceTempView(\"toptypeevolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82a793-9c05-4e1d-8af5-a2f8baeec510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rowsavant = []\n",
    "for pred in topavant_list:\n",
    "    print(pred)\n",
    "    rowsavant.append(Row(type=pred, average=float(calculate_average(\"CSET_avant\", pred)), coverage=float(calculate_coverage(\"CSET_avant\", pred))) )\n",
    "    #print(rowsavant)\n",
    "\n",
    "dfavant = spark.createDataFrame(rowsavant)\n",
    "dfavant.createOrReplaceTempView(\"avcovavant\")\n",
    "\n",
    "rowsapres = []\n",
    "for pred in topapres_list:\n",
    "    print(pred)\n",
    "    rowsapres.append(Row(type=pred, average=float(calculate_average(\"CSET_apres\", pred)), coverage=float(calculate_coverage(\"CSET_apres\", pred))) )\n",
    "    #print(rowsapres)\n",
    "\n",
    "dfapres = spark.createDataFrame(rowsapres)\n",
    "dfapres.createOrReplaceTempView(\"avcovapres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ac8c8-8a8d-4aa8-b563-45e5a613f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from avcovavant\").show(truncate=0)\n",
    "spark.sql(\"select * from avcovapres\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6966ca-e68c-4366-b362-a553068507d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiche l'evolution du coverage et de l'average pour chacun des types\n",
    "covavevolution = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COALESCE(avcovavant.type, avcovapres.type) AS type,\n",
    "    avcovavant.average AS average_before,\n",
    "    avcovapres.average AS average_after,\n",
    "    CASE\n",
    "        WHEN avcovavant.type IS NULL THEN \"This type is only in the after graph\"  -- Type is only in avcovapres\n",
    "        WHEN avcovapres.type IS NULL THEN \"This type is only in the before graph\"  -- Type is only in avcovavant\n",
    "        ELSE (avcovapres.average - avcovavant.average) / avcovavant.average * 100.0\n",
    "    END AS percentage_average_evolution,\n",
    "    avcovavant.coverage AS coverage_before,\n",
    "    avcovapres.coverage AS coverage_after,\n",
    "    CASE\n",
    "        WHEN avcovavant.type IS NULL THEN \"This type is only in the after graph\"  -- Type is only in avcovapres\n",
    "        WHEN avcovapres.type IS NULL THEN \"This type is only in the before graph\"  -- Type is only in avcovavant\n",
    "        ELSE (avcovapres.coverage - avcovavant.coverage) / avcovavant.coverage * 100.0\n",
    "    END AS percentage_coverage_evolution\n",
    "FROM\n",
    "    avcovavant\n",
    "FULL OUTER JOIN\n",
    "    avcovapres\n",
    "ON\n",
    "    avcovavant.type = avcovapres.type\n",
    "\"\"\")\n",
    "covavevolution.show(truncate=0)\n",
    "covavevolution.createOrReplaceTempView(\"covavevolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f082d1-ea61-4bbf-bd5c-e05875b745b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = spark.sql(\"\"\"\n",
    "select * \n",
    "from covavevolution \n",
    "FULL OUTER JOIN \n",
    "    toptypeevolution\n",
    "ON\n",
    "    toptypeevolution.type = covavevolution.type\"\"\")\n",
    "combine.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9360be-fce9-4d7d-950c-498921394225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4ff28-4dd0-4676-9d8e-541e85b9812e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
