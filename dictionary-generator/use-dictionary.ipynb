{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166726a6-d97d-43e7-88e1-732016e8f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 19:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/27 19:24:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName(\"WDC-DataFrame\").getOrCreate()\n",
    "#créer une session dans le master\n",
    "spark = SparkSession.builder.master(\"spark://172.20.53.96:7077\").appName(\"WDC-use-dico\").getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"WDC-use-dico\").getOrCreate()\n",
    "\n",
    "\n",
    "#fichiers de config qui permettent de se connecter au serveur de stockage s3 qui contient les fichiers de DataCommons\n",
    "endpoint_url = 'https://s3.os-bird.glicid.fr/'\n",
    "aws_access_key_id = 'bbd95ea3c1174caa88345404b84e458f'\n",
    "aws_secret_access_key = 'eaf2a72ecf9845f583af7f3513c44f25'\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', aws_access_key_id)\n",
    "hadoopConf.set('fs.s3a.secret.key', aws_secret_access_key)\n",
    "hadoopConf.set('fs.s3a.endpoint', endpoint_url)\n",
    "hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "hadoopConf.set('spark.worker.cleanup.enabled', 'true')\n",
    "hadoopConf.set('fs.s3a.committer.name', 'magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50023e02-b993-4c09-95c5-a22be98b4f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 19:24:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+--------------------------------------+\n",
      "|subject                                                                              |predicate                             |\n",
      "+-------------------------------------------------------------------------------------+--------------------------------------+\n",
      "|<http://148.163.67.237/#website>                                                     |<w3.org/1999/02/22-rdf-syntax-ns#type>|\n",
      "|<http://148.163.67.237/#website>                                                     |<schema.org/description>              |\n",
      "|<http://148.163.67.237/#website>                                                     |<schema.org/inLanguage>               |\n",
      "|<http://148.163.67.237/#website>                                                     |<schema.org/name>                     |\n",
      "|<http://148.163.67.237/#website>                                                     |<schema.org/potentialAction>          |\n",
      "|<http://148.163.67.237/#website>                                                     |<schema.org/url>                      |\n",
      "|_:ne22bb1dfb7b445a2af60a0d8a5da5770xb0                                               |<w3.org/1999/02/22-rdf-syntax-ns#type>|\n",
      "|_:ne22bb1dfb7b445a2af60a0d8a5da5770xb0                                               |<schema.org/query-input>              |\n",
      "|_:ne22bb1dfb7b445a2af60a0d8a5da5770xb0                                               |<schema.org/target>                   |\n",
      "|<http://148.163.67.237/>                                                             |<w3.org/1999/02/22-rdf-syntax-ns#type>|\n",
      "|<http://148.163.67.237/>                                                             |<schema.org/name>                     |\n",
      "|<http://148.163.67.237/>                                                             |<schema.org/url>                      |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<w3.org/1999/02/22-rdf-syntax-ns#type>|\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<schema.org/caption>                  |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<schema.org/height>                   |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<schema.org/inLanguage>               |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<schema.org/url>                      |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#primaryimage>|<schema.org/width>                    |\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#webpage>     |<w3.org/1999/02/22-rdf-syntax-ns#type>|\n",
      "|<http://148.163.67.237/2019/01/skandal-perselingkuhan-dokter-vs-pasien/#webpage>     |<schema.org/author>                   |\n",
      "+-------------------------------------------------------------------------------------+--------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "df = spark.read.text(\"s3a://test/\")\n",
    "\n",
    "#Split the line into four parts using the ' ' separator\n",
    "split_col = f.split(f.col(\"value\"), \"\\s+\", 3)\n",
    "\n",
    "#Create new columns for the subject, predicate, object, and graph\n",
    "df = df.withColumn(\"subject\", split_col[0])\n",
    "df = df.withColumn(\"predicate\", split_col[1])\n",
    "df = df.drop(\"value\")\n",
    "\n",
    "# permet de ne plus différencier les predicats http, https, www...\n",
    "df = df.withColumn(\"predicate\", f.regexp_replace(f.col(\"predicate\"), \"([Hh][Tt][Tt][Pp][Ss]?://)?([Ww]{3}\\.)?\", \"\"))\n",
    "df.show(truncate=0)\n",
    "\n",
    "#df.createOrReplaceTempView(\"Super\")\n",
    "#spark.sql(\"select count(*) as count from Super\").show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292d1099-6bd0-4488-981a-27686615d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (43 + 1) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   27865|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Charge le dictionnaire\n",
    "dico = spark.read.option(\"header\",True) \\\n",
    "  .csv(\"s3a://test-out/dico_reel_total\")\n",
    "dico.createOrReplaceTempView(\"Dico\")\n",
    "spark.sql(\"select count(*) from Dico\").show()\n",
    "#dico.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b9f3eb-5469-4ef5-aa57-5ecd61b0f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|predicate|             subject|\n",
      "+---------+--------------------+\n",
      "|        0|<http://148.163.6...|\n",
      "|        7|<http://148.163.6...|\n",
      "|        8|<http://148.163.6...|\n",
      "|        1|<http://148.163.6...|\n",
      "|       10|<http://148.163.6...|\n",
      "|        2|<http://148.163.6...|\n",
      "|        0|_:ne22bb1dfb7b445...|\n",
      "|       22|_:ne22bb1dfb7b445...|\n",
      "|       11|_:ne22bb1dfb7b445...|\n",
      "|        0|<http://148.163.6...|\n",
      "|        1|<http://148.163.6...|\n",
      "|        2|<http://148.163.6...|\n",
      "|        0|<http://148.163.6...|\n",
      "|       24|<http://148.163.6...|\n",
      "|       13|<http://148.163.6...|\n",
      "|        8|<http://148.163.6...|\n",
      "|        2|<http://148.163.6...|\n",
      "|       12|<http://148.163.6...|\n",
      "|        0|<http://148.163.6...|\n",
      "|       16|<http://148.163.6...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Attribut les id du dictionnaire au df\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join df et dico sur la colomne \"predicate\"\n",
    "joined_df = df.join(dico, on=\"predicate\", how=\"left\")\n",
    "\n",
    "# Remplace la colomne \"id\" par \"predicate\"\n",
    "final_df = joined_df.withColumn(\"predicate\", col(\"id\")).drop(\"id\")\n",
    "final_df.createOrReplaceTempView(\"Super\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18355d4-296d-457f-860a-a5e06f193ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Mesure le temps d'exécution du characteristic set\n",
    "\n",
    "# Vieille implem par Set, pas stockable en csv\n",
    "#result=final_df.groupby(\"subject\").agg(f.sort_array(f.collect_set(final_df.predicate)).alias(\"pset\"))\n",
    "#result2=result.groupby(\"pset\").agg(f.approx_count_distinct(result.subject).alias('count')).sort(f.desc(\"count\"))\n",
    "#result2.show(truncate=0)\n",
    "\n",
    "cset = spark.sql(f\"select subject, concat_ws('|',sort_array(collect_set(predicate))) as pset FROM Super group by  subject \").cache()\n",
    "\n",
    "result = cset.groupby(\"pset\").agg(f.count(cset.subject).alias('count')).sort(f.desc('count'))\n",
    "#result2=result.groupby(\"pset\").agg(f.approx_count_distinct(result.subject).alias('count')).sort(f.desc(\"count\"))\n",
    "result.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508f33f-a1fe-473a-8109-58332b65d8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
