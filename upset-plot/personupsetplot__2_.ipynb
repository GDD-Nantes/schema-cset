{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afj-AjKjC4II"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "#cr√©er une session dans le master\n",
        "spark = SparkSession.builder.master(\"spark://172.20.53.96:7077\").appName(\"WDC-readAll\").getOrCreate()\n",
        "#spark = SparkSession.builder.master(\"local\").appName(\"WDC-readAll\").getOrCreate()\n",
        "\n",
        "#fichiers de config qui permettent de se connecter au serveur de stockage s3 qui contient les fichiers de DataCommons\n",
        "endpoint_url = 'https://s3.os-bird.glicid.fr/'\n",
        "aws_access_key_id = '***REMOVED***'\n",
        "aws_secret_access_key = '***REMOVED***'\n",
        "hadoopConf = spark._jsc.hadoopConfiguration()\n",
        "hadoopConf.set('fs.s3a.access.key', aws_access_key_id)\n",
        "hadoopConf.set('fs.s3a.secret.key', aws_secret_access_key)\n",
        "hadoopConf.set('fs.s3a.endpoint', endpoint_url)\n",
        "hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
        "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
        "\n",
        "hadoopConf.set('spark.worker.cleanup.enabled', 'true')\n",
        "hadoopConf.set('fs.s3a.committer.name', 'magic')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "readall = spark.read.option(\"header\",True) \\\n",
        "  .csv(\"s3a://test-out/wdc-httpswww/**\")\n",
        "\n",
        "readall.take(5)\n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "csall=readall.groupby(\"pset\").agg(f.sum(\"count\").alias('count')).sort(f.desc(\"count\"))\n",
        "csall.show(truncate=0)\n",
        "csall.createOrReplaceTempView(\"CSET\")"
      ],
      "metadata": {
        "id": "EOsQGAkJDAiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persons=spark.sql(\"select pset,count from CSET where pset like '%isa:<schema.org/Person>%'\")"
      ],
      "metadata": {
        "id": "aBDRDXBeDaW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#turn csets into lists\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "persons = persons.withColumn(\"pset\", split(persons[\"pset\"], \"\\\\|\"))\n",
        "persons.show(10, truncate=80)"
      ],
      "metadata": {
        "id": "yHgGomp3De9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persons.createOrReplaceTempView(\"persons\")\n",
        "top_persons= spark.sql(\"\"\"\n",
        "    SELECT  *\n",
        "    FROM persons\n",
        "    limit 15\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "9Ok5u5n7Do1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install UpSetPlot\n",
        "!pip install matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import upsetplot\n",
        "\n",
        "pandas_persons = top_persons.toPandas()\n",
        "\n",
        "# Create a list of sets for each row in the DataFrame\n",
        "set_list = [set(x) for x in pandas_persons['pset']]\n",
        "\n",
        "# Create the UpSetplot\n",
        "upset_data = upsetplot.from_memberships(set_list, data=pandas_persons['count'])\n",
        "upsetplot.plot(upset_data)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "27bDY0stDumB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
