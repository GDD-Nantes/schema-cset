{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafaec70-ffb4-41c6-80cc-c9507e68fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 20:25:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/08 20:25:48 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#créer une session dans le master\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://172.20.53.96:7077\") \\\n",
    "    .appName(\"WDC-complete\") \\\n",
    "    .config(\"spark.executor.memory\",\"20g\") \\\n",
    "    .config(\"spark.driver.memory\",\"20g\") \\\n",
    "    .getOrCreate()\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"WDC-complete\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",1000)\n",
    "\n",
    "#fichiers de config qui permettent de se connecter au serveur de stockage s3 qui contient les fichiers de DataCommons\n",
    "endpoint_url = 'https://s3.os-bird.glicid.fr/'\n",
    "aws_access_key_id = 'bbd95ea3c1174caa88345404b84e458f'\n",
    "aws_secret_access_key = 'eaf2a72ecf9845f583af7f3513c44f25'\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', aws_access_key_id)\n",
    "hadoopConf.set('fs.s3a.secret.key', aws_secret_access_key)\n",
    "hadoopConf.set('fs.s3a.endpoint', endpoint_url)\n",
    "hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "hadoopConf.set('spark.worker.cleanup.enabled', 'true')\n",
    "hadoopConf.set('fs.s3a.committer.name', 'magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f044a-e5d2-49a9-8fb1-b96443752e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(subject='_:nb53a24408607424384c1357880ce1bc7xb1', predicate='<http://schema.org/value>', prov='peakery.com', hashdom=8)\n",
      "23/04/08 20:26:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-----------------------------+-----------------+-------+\n",
      "|subject                                 |predicate                    |prov             |hashdom|\n",
      "+----------------------------------------+-----------------------------+-----------------+-------+\n",
      "|<http://1000lifehacks.com/#organization>|isa:<schema.org/Organization>|1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/image>           |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/logo>            |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/name>            |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/sameAs>          |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/sameAs>          |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/sameAs>          |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#organization>|<schema.org/url>             |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |isa:<schema.org/ImageObject> |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |<schema.org/caption>         |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |<schema.org/height>          |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |<schema.org/inLanguage>      |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |<schema.org/url>             |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#logo>        |<schema.org/width>           |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |isa:<schema.org/WebSite>     |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |<schema.org/description>     |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |<schema.org/inLanguage>      |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |<schema.org/name>            |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |<schema.org/potentialAction> |1000lifehacks.com|1      |\n",
      "|<http://1000lifehacks.com/#website>     |<schema.org/publisher>       |1000lifehacks.com|1      |\n",
      "+----------------------------------------+-----------------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "line='_:nb53a24408607424384c1357880ce1bc7xb1 <http://schema.org/value> \"8,840 ft / 2,694 m\" <https://peakery.com/dragontail-peak-washington/>   .'\n",
    "quad_motif=re.compile(r'(.+)\\s(<.+?>)\\s(.+)<(.+)>\\s+.')\n",
    "\n",
    "def parseQ(l):\n",
    "  result=quad_motif.match(l)\n",
    "  #print(result.groups())\n",
    "  sub=result.group(1).strip()\n",
    "  pred=result.group(2).strip()\n",
    "  if pred==\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>\":\n",
    "      pred=\"isa:\"+result.group(3).strip()\n",
    "  domain=urlparse(result.group(4).strip()).netloc\n",
    "  return Row(subject=sub,predicate=pred,prov=domain,hashdom=hash(domain)%10)\n",
    "\n",
    "print(parseQ(line))\n",
    "\n",
    "#lines = spark.sparkContext.textFile(\"s3a://test/\")\n",
    "lines = spark.sparkContext.textFile(\"s3a://wdc/\")\n",
    "\n",
    "sp=lines.map(lambda l: parseQ(l)).toDF()\n",
    "\n",
    "# permet de ne plus différencier les predicats http, https, www...\n",
    "sp = sp.withColumn(\"predicate\", f.regexp_replace(f.col(\"predicate\"), \"([Hh][Tt][Tt][Pp][Ss]?://)?([Ww]{3}\\.)?\", \"\"))\n",
    "sp.show(truncate=0)\n",
    "\n",
    "sp.createOrReplaceTempView(\"Super\")\n",
    "#spark.sql(\"select count(*) as count from Super\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b73dfe-567b-44dc-836f-33021f534f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>(6232 + 8) / 6240]\r"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    hashdom_val = i\n",
    "    file_name = f\"hashdom{hashdom_val+1}-10\"\n",
    "    cset2 = spark.sql(f\"select subject, concat_ws('|',sort_array(collect_set(predicate))) as pset FROM Super where hashdom={hashdom_val} group by  subject \").cache()\n",
    "    cset2.show(truncate=200)\n",
    "    print(cset2.count())\n",
    "\n",
    "    result2 = cset2.groupby(\"pset\").agg(f.count(cset2.subject).alias('count'))\n",
    "    result2.show(truncate=0)\n",
    "\n",
    "    result2.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(f\"s3a://test-out/newwdc/{file_name}\")\n",
    "    \n",
    "    # clear variables from memory\n",
    "    cset2.unpersist()\n",
    "    result2.unpersist()\n",
    "    del cset2, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07799c24-7c9e-43e0-844d-aa2fc3e7b280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
