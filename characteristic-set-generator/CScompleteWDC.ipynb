{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885330be-eafa-426c-b731-92d6346fa284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 21:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/13 21:44:44 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/04/13 21:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/13 21:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/13 21:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/13 21:44:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#cr√©er une session dans le master\n",
    "#spark = SparkSession.builder \\\n",
    "#    .master(\"spark://172.20.53.96:7077\") \\\n",
    "#    .appName(\"WDC-complete\") \\\n",
    "#    .config(\"spark.executor.memory\",\"28g\") \\\n",
    "#    .config(\"spark.driver.memory\",\"28g\") \\\n",
    "#    .getOrCreate()\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"WDC-complete\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",1000)\n",
    "\n",
    "#fichiers de config qui permettent de se connecter au serveur de stockage s3 qui contient les fichiers de DataCommons\n",
    "endpoint_url = 'https://s3.os-bird.glicid.fr/'\n",
    "aws_access_key_id = 'bbd95ea3c1174caa88345404b84e458f'\n",
    "aws_secret_access_key = 'eaf2a72ecf9845f583af7f3513c44f25'\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', aws_access_key_id)\n",
    "hadoopConf.set('fs.s3a.secret.key', aws_secret_access_key)\n",
    "hadoopConf.set('fs.s3a.endpoint', endpoint_url)\n",
    "hadoopConf.set('fs.s3a.path.style.access', 'true')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "hadoopConf.set('spark.worker.cleanup.enabled', 'true')\n",
    "hadoopConf.set('fs.s3a.committer.name', 'magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b893de1a-8fa9-4382-b331-8f59c2a16e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(subject='_:nb53a24408607424384c1357880ce1bc7xb1', predicate='<http://schema.org/value>', prov='peakery.com', hashdom=6)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "line='_:nb53a24408607424384c1357880ce1bc7xb1 <http://schema.org/value> \"8,840 ft<> / 2,694 m<>\" <https://peakery.com/dragontail-peak-washington/>   .'\n",
    "quad_motif = re.compile(r'([^\\s]+)\\s([^\\s]+)\\s(.+)\\s([^\\s]+)\\s+\\.')\n",
    "\n",
    "def parseQ(l, parts):\n",
    "  result=quad_motif.match(l)\n",
    "  #print(result.groups())\n",
    "  sub=result.group(1).strip()\n",
    "  pred=result.group(2).strip()\n",
    "  if pred==\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>\":\n",
    "      pred=\"isa:\"+result.group(3).strip()\n",
    "  domain=urlparse(result.group(4).strip().strip(\"<>\")).netloc\n",
    "  return Row(subject=sub,predicate=pred,prov=domain,hashdom=hash(domain)%parts)\n",
    "\n",
    "print(parseQ(line, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22a0e88-774a-4136-b07f-422954618be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partCS(hashdom_val, hashdom_max, output=None):\n",
    "    print(f\"part {hashdom_val+1}/{hashdom_max} started\");\n",
    "    file_name = f\"hashdom{hashdom_val+1}-{hashdom_max}\"\n",
    "    cset2 = spark.sql(f\"select subject, concat_ws(' ',sort_array(collect_set(predicate))) as pset FROM Super where hashdom={hashdom_val} group by  subject \").cache()\n",
    "    #cset2.show(truncate=200)\n",
    "    #print(cset2.count())\n",
    "\n",
    "    result2 = cset2.groupby(\"pset\").agg(f.count(cset2.subject).alias('count'))\n",
    "    result2.show(truncate=0)\n",
    "\n",
    "    if(output is not None):\n",
    "        print(\"Saving\")\n",
    "        result2.write.option(\"header\",True) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .csv(f\"s3a://test-out/{output}/{file_name}\")\n",
    "    \n",
    "    # clear variables from memory\n",
    "    cset2.unpersist()\n",
    "    result2.unpersist()\n",
    "\n",
    "    print(f\"part {hashdom_val+1}/{hashdom_max} finished\");\n",
    "    \n",
    "    del cset2, result2\n",
    "\n",
    "def completeCS(input, parts, output=None):\n",
    "    lines = spark.sparkContext.textFile(f\"s3a://{input}/\")\n",
    "    \n",
    "    sp=lines.map(lambda l: parseQ(l, parts)).toDF()\n",
    "\n",
    "    sp = sp.withColumn(\"predicate\", f.regexp_replace(f.col(\"predicate\"), \"([Hh][Tt][Tt][Pp][Ss]?://)?([Ww]{3}\\.)?\", \"\"))\n",
    "    sp.show(truncate=0)\n",
    "\n",
    "    sp.createOrReplaceTempView(\"Super\")\n",
    "    \n",
    "    for i in range(parts):\n",
    "        partCS(i, parts, output)\n",
    "        \n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf2537-9906-4157-891e-701663c6ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=========>         (1 + 1) / 2][Stage 9:>                  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "# input : test or wdc\n",
    "# no output = no save\n",
    "completeCS(\"test\", 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
